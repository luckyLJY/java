查看软件安装目录：which mysqld

先检查下mysql是否在自启动列表

chkconfig --list mysqld


将mysql加入自启动列表

chkconfig --level 345 mysqld on

远程复制命令

scp -r hbase-2.4.6/ root@node03:/export/servers/



新建文件：vim FileName,cat >> FileName,touch FileName

查看目录下文件大小：ls -lht  ;du -sh *

```
df -hl 查看磁盘剩余空间
```

```
df -h 查看每个根路径的分区大小
```

```
du -sh [目录名] 返回该目录的大小
```

```
du -sm [文件夹] 返回该文件夹总M数
```

```
du -h [目录名] 查看指定文件夹下的所有文件大小（包含子文件夹）
```

netstat -anp |grep [端口号] 查看端口占用

nc -lk [端口号] 监听端口号

```shell
#!/bin/bash
f(){
DIR=`ls -l|grep "^d"|awk '{print $9}'`
if [ ! -n "$DIR" ]; then
        echo "IS NULL"
else
        for i in $DIR
                do

                        DIR1=`pwd`"/"
                        DIR3=${DIR1}${i}
                        DIR4=`cd ${DIR3}`
                        echo "目录为：$i"
                        DIR2=`cd ${DIR3}|grep ls -l|grep "^d"|awk '{print $9}'`
                        if [ ! -n "$DIR2" ]; then
                                echo""
                        else
                                f
                        fi
                done
fi
}
f
```

netstat -ano|grep 7777 //查看端口

ps -ef //查看进程

### orcal

#### 修改数据、及删除表

```sql
//修改数据
select r.*,rowid from tmp_invm r;
//删除表
drop table table_name;
```

### JAVA

#### 数组转列表

```java
List<String> stooges = Arrays.asList("Larry", "Moe", "Curly"); 
```



### Flink

#### 基本安全性配置

```java
flink:
    isCheck：是否设置检查点
    checkTime:检查指定检查点启动间隔时间enableCheckpointing
    CheckpointingMode:检查点模式
    windowTime:窗口大小
    watermarkTime: 时间水平线
    laterTime:消息迟到时间
    batchCount:消息窗口内的消息条数
    windowslidewindow:窗口移动偏移量

kafka
	topics:消息主题
    boostrapservers:zookeeper服务
    Constant.GROUP.ID用户组
    ConsumerConfig.KAFKA_AUTO_OFFSET_REST:从什么地方开始读数据：开始，最新
    ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG:偏移量自动提交true
    
```

#### 间隔读取数据源

​	实现SourceFunction接口：在run方法中Thread.sleep(时间间隔)

#### 读取参数

```
ParameterTool.fromArgs(args);//main方法中携带参数
ParameterTool的对象.getRequired(配置常量类.参数名称)//读取配置常量参数
```

### es操作

#### es修改字段属性

```json
PUT 索引/doc/_mapping/
{
  "properties": {
    "city": { 
      "type":     "text",
      "fielddata": true
    }
  }
}

//curl命令
curl -H 'Content-Type: application/json' -XPUT "http://192.168.31.203:9200/dcs_new"
curl  -H 'Content-Type: application/json' -XPOST "http://192.168.31.203:9200/dcs_new/qzwl_dcs/_mapping?pretty" -d ' 
{
    "qzwl_dcs": {
            "properties": {
                "value": {
                    "type": "double",
                    "store": "true",
                    "ignore_malformed": "true"
                }
            }
        }
  }

```

#### curl操作es命令

#### 	集群常用命令

- 查看命令

```shell
curl  -XGET 'http://hadoop137:9200'
```

- 查看集群状态

```shell
curl -XGET 'http://hadoop137:9200/_cluster/state?pretty'

#这里在url后面添加了pretty是为了让其在控制台上输出的结果是一个优美的json格式
```

##### 索引库常用命令

- 查看所有索引信息

```shell
curl -XGET 'http://hadoop137:9200/_cat/indices?pretty&v'
```

- 创建索引

```shell
curl -XPUT 'http://hadoop137:9200/upuptop?pretty'
```

- 删除索引

```shell
curl -XDELETE 'http://hadoop137:9200/upuptop?pretty'
```

##### 文档常用命令

- 创建文档

```shell
# 9200/索引库名/文档类型/id/  -d 文档内容
# id可以忽略，ES会自动生成id，如果id存在，那么就是更新数据，字段可以增加

curl -XPOST 'http://hadoop137:9200/upuptop/stu/1?pretty' -H 'Content-Type: application/json' -d '
{
"name":"shaofei",
"age":22,
"sex":1
}'

```

- 修改文档

```shell
# 给id为1的学生修改姓名为upuptop，年龄修改为25，添加学号字段
curl -XPOST 'http://hadoop137:9200/upuptop/stu/1?pretty' -H 'Content-Type: application/json' -d '
{
"name":"upuptop",
"age":25,
"sex":0,
"number":"1501260222"
}'
```

- 查看所有文档

```shell
curl -XGET 'http://hadoop137:9200/upuptop/stu/_search?pretty'
```

- 根据id查看文档

```shell
curl -XGET 'http://hadoop137:9200/upuptop/stu/1?pretty'
```

- 删除文档

```shell
curl -XDELETE 'http://hadoop137:9200/upuptop/stu/1?pretty'
```

#### POSTMAN操作

##### es对多个字段求和

```sql
select sum(value_name),sum(value1_name) from index
```



```json
POST index/_search
{
    "aggs":{
        "value_name":{
            "sum":{
                "field":"OTHER_CHANNEL_COUNT"
            }
        },
        "value1_name":{
            "sum":{
                "field":"OTHER_CHANNEL_AMOUNT"
            }
        }
    }
}
```

##### es查询多个字段

```sql
select value1,value2 from index	where datetime >=now
```



```
POST index/_search?
{
	"_source":[_value1,value2],
	"query":{
		"range":{
			"datatime":{
				"gte":now,
				"format":"ddMMyyyy"
			}
		}
	}
}
```

##### es排序求第行数据

```sql
select * from index order by _id limit 1
```



```json
GET index/_search
{
    "sort":[
        {
            "_id":{
                "order":desc
            }
        }
    ],"size":1
}
```

##### 排序

```sql
select * from index order by hours
```

```json
GET index/_search
{
    "sort:[
    	{
    		"hours.keyword":{
    			"order":desc
			}
		}
    ]"
}

--这里的排序字段需要使用：字段.keyword
```



### idea

any rule插件使用快捷键alt+a

Tranlation:英文翻译插件：ctrl+shit+Y

### java

#### token和session的区别

```txt
session的弊端
	1.服务器资源占用大：存储在服务器的内存中，随着用户量的增加，服务器压力增大；
	2.安全低：基于cookie进行用户识别；
	3.推展性不强：session的数据保存在单节点上，用户再次登陆可能无法获取session;
	
token:通过服务器计算token进行用户验证
```

#### get和post请求

```txt
get请求：
	1.参数位置在URL中；
	2.能够被浏览器缓存;(2048B)
	3.参数长度有限；
	4.参数安全新低；
	5.支持浏览器访问
post请求：
	1.参数在请求体中；
	2.不能被浏览器缓存;
	3.参数长度不受限;
	4.参数安全性较好;
	5.不支持浏览器访问;
```

### liunx

#### 网络配置

```shell
1.vmare中"编辑"->虚拟网络编辑器->NAT8->更改设置->NAT8->修改子网->nat网关.2
2.wind10网络 外部dns为8.8.8.8
3.vim /etc/sysconfig/network-scripts/ifcfg-ens33
TYPE="Ethernet"
PROXY_METHOD="none"
BROWSER_ONLY="no"
BOOTPROTO="static"
DEFROUTE="yes"
IPV4_FAILURE_FATAL="no"
IPV6INIT="yes"
IPV6_AUTOCONF="yes"
IPV6_DEFROUTE="yes"
IPV6_FAILURE_FATAL="no"
IPV6_ADDR_GEN_MODE="stable-privacy"
NAME="ens33"
UUID="1ae3a9d0-0d49-4fbe-9d8e-07d61e056ebc"
DEVICE="ens33"
ONBOOT="yes"
#IP
IPADDR=192.168.75.200
GATEWAY=192.168.75.2
DNS1=192.168.75.2
4.vim /etc/hostname
5.vim /etc/hosts
6.关闭防火墙：
	systemctl stop firewalld
	systemctl disable firewalld.service
```

#### 远程连接

```shell
配置本级host文件
192.168.75.200 hadoop100
192.168.75.201 hadoop101
192.168.75.202 hadoop102
192.168.75.203 hadoop103
192.168.75.204 hadoop104
192.168.75.205 hadoop105
192.168.75.206 hadoop106
192.168.75.207 hadoop107
192.168.75.208 hadoop108
```

#### scp与xsync

```shell
scp:安全拷贝
rsync -r $pdir/$fname $user@host:$pdir/$fname
rsync:镜像同步
rsync -av $pdir/$fname $user@host:$pdir/$fname
xsync集群分发脚本

```



#### 删除原始java文件

```shell
#删除原始java文件
rpm -qa|grep -i java |xargs -n1 rpm -e --nodeps
#查看原始java文件
rpm -qa|grep -i java
```



#### 日志查看

```txt
命令有tail、head、cat、less等等
	cat a.log #显示a.log的整个文件内容,内容较少
	more 1.log #查看内容较多的文件，使用空格键来翻页
	less 1.log #查看内容较多的文件，使用上下键翻页
	head  #查看日志文件的前n行内容
	tail -f 1.log#查看1.log日志文件的后10行文件内容

tail -n 1000 文件 重定向到其他文件

cat file| grep -E "条件" >> file.log
```

#### 查找文件

```shell
# 全局查找abc.log的文件路径
find / -name abc.log
locate abc.log
```

#### vi文件中部分字符替换

```shell
:%/要替换的字符/替换成的字符
```

#### 系统任务调度crontab

```shell
/etc/crontab文件包括下面几行：

SHELL=/bin/bash
PATH=/sbin:/bin:/usr/sbin:/usr/bin
MAILTO=""HOME=/
#有些需要执行的命令的环境变量

# run-parts
51 * * * * root run-parts /etc/cron.hourly
24 7 * * * root run-parts /etc/cron.daily
22 4 * * 0 root run-parts /etc/cron.weekly
42 4 1 * * root run-parts /etc/cron.monthly

minute   hour   day   month   week   command     顺序：分 时 日 月 周
```



### 系统架构

#### 集群、分布式、微服务的概念和区别

```txt

```

### kafka

## 数据推送

- ​	需要设置批量数据的属性：isBatchListener;
- 逐条消费时需要配置list，需要配置内

#### 消息操作命令

~~~shell
Kafka命令
--创建topic
	```shell
	./kafka-topics.sh --create --boostrap-server localhost:9092 --replication-factor 1 --patitions 1 --topic test
	```

		--create： 指定创建topic动作

		--topic：指定新建topic的名称

		--zookeeper： 指定kafka连接zk的连接url，该值和server.properties文件中的配置项{zookeeper.connect}一样

		--config：指定当前topic上有效的参数值，

		--partitions：指定当前创建的kafka分区数量，默认为1个

		--replication-factor：指定每个分区的复制因子个数，默认1个

--查看topic

	```shell
	./kafka-topics.sh --list --boostrap-server localhost:9092
	```

--发送消息

	```shell
	./kafka-console-producer.sh --broker-list localhost:9092 -topic test
	```

--消费消息

	```shell
	./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning
	```

--删除topic

	```shell
	./kafka-topic --delete --boostrap-server localhost:9092 --topic test
	```

~~~



### hive

#### 数值字段转化

```sql
case
  --处理非科学计数法表示的字符串
  when length(regexp_extract('字符串','([0-9]+\\.)([0-9]+)(E-*[0-9]+)',2))=0
then '字符串'
  --处理整数
  when length(regexp_extract('字符串','([0-9]+\\.)([0-9]+)(E[0-9]+)',2))<=cast(regexp_extract('字符串','(E)([0-9]+)',2) as int)
    then rpad(regexp_replace(regexp_extract('字符串','([^E]+)',1),'\\.',''),cast(regexp_extract('字符串','(E)([0-9]+)',2) as int)+1,'0')
  --处理小数
  when length(regexp_extract('字符串','([0-9]+\\.)([0-9]+)(E[0-9]+)',2))>cast(regexp_extract('字符串','(E)([0-9]+)',2) as int)
    then concat(substr(regexp_replace(regexp_extract('字符串','([^E]+)',1),'\\.',''),1,cast(regexp_extract('字符串','(E)([0-9]+)',2) as int)+1),'\.',
    substr(regexp_replace(regexp_extract('字符串','([^E]+)',1),'\\.',''),cast(regexp_extract('字符串','(E)([0-9]+)',2) as int)+2))
  --处理类似“3.4E-6”这种字符串
  when '字符串' regexp 'E-'
    then concat('0.',repeat('0',cast(regexp_extract('字符串','(E)(-)([0-9]+)',3) as int)-1),regexp_replace(regexp_extract('字符串','(.+)(E)',1),'\\.',''))
  else '字符串'
end
```

#### localtask超轻量的job在提交前session中关闭自动mapjoin

```shell
解决：set hive.auto.convert.join=false
```

### hbase

#### scan读取一行数据

```shell
--查询tablename的一条数据
scan 'tablename', LIMIT => 1
--查询user表中的所有信息
scan 'user'
--列族查询
scan 'user',{COLUMNS => 'info',RAW => true,VERSIONS => 5}
--多列族查询
scan 'user',{COLUMNS => ['info','data']}
scan 'user',{COLUMNS => ['info:name','data:pic']}
--指定列族和列名，以及限定的版本查询
scan 'user',{COLUMNS => 'info:name',VERSIONS => 5}
--按照数据值指定多个列族进行模糊查询
scan 'user',{COLUMNS => ['info','data'],FILTER => "(QualifierFilter(=,'substring:a'))"}
--rowkey的范围值查询
scan 'user',{COLUMNS => 'info',STARTROW => 'rk001',ENDROW => 'rk003'}
--rowkey模糊查询
scan 'user',{FILTER => "PrefixFilter('rk')"}
--指定数据范查询
scan 'user',{TIMERANGE => [1,2]}

```

#### get、put基本命令

```shell
--查看帮助命令
help
--查看当前数据库中有哪些表
list

--创建一张表
--创建user表，包含info、data两个列族
create 'user','info','data'
create 'user', {NAME => 'info',VERSIONS => '3'},{NAME => 'data'}

--添加数据操作
--将信息表插入user表中，row key为rk001,在info列中添加name列标记符，值为zhangsn
put 'user','rk001','info:name','zhangsan'
--将信息插入到user表，row key为rk001,将pic列标识符添加到列族data，值为picture
put 'user','rk001','data:pic','picture'

--查询数据操作
--获取user表中row key为rk001的所有信息
get 'user','rk001'
--查看user表中rowk key为rk001,info列族的所有信息
get 'user','rk001','info'
--查看user表中row key为rk001,info列族的name、age列表符的信息
get 'user','rk001','info:name','info:age'
--在user表中获得row key为rk001，info、data列的信息
get 'user','rk001','info','data'
get 'user','rk001',{COLUMN => ['info','data']}
get 'user','rk001',{COLUMN => ['info:name','data:pic']}
--获得用户表中row key为rk001,而cell值为zhangsan的信息
get 'user','rk001',{FLTER => "(QualifierFilter(=,'substring:a'))"}
--指定row key和列值模糊查询
```

#### 更新数据值

```shell
--更新版本号
alter 'user',NAME => 'info',VERSIONS => 5
```

#### 删除操作

```shell
--指定rowkey以及列表进行删除
delete 'user','rk001','info:name'
--指定rowkey，列名以及字段值进行删除
delete 'user','rk001','info:name',123
--删除一个列族
alter 'user',NAME => 'info',METHOD =>'delete'
alter 'user','delete' => 'info'
--清空表数据
truncate 'user'
```

#### 移除表

```shell
drop 'user'
```

#### Count操作

```shell
count 'user'
```



### yarn操作命令

#### 任务管理命令

```shell
--查看任务
yarn application -list
--根据Application状态过滤：yarn application -list -appStates （所有状态：ALL、NEW、NEW_SAVING、SUBMITTED、ACCEPTED、RUNNING、FINISHED、FAILED、KILLED）\
yarn application -list -appStates FINISHED
--杀掉任务
yarn application -kill application_1612577921195_0001
--列出所有Application尝试的列表：
yarn applicationattempt -list <ApplicationId>
--打印ApplicationAttemp状态：
yarn applicationattempt -status <ApplicationAttemptId>
--列出所有Container：
yarn container -list <ApplicationAttemptId>
--打印Container状态：	
yarn container -status <ContainerId>
--列出所有节点：
yarn node -list -all
```



### 启动命令

#### zookeeper命令


```shell
zookeeper启动命令 		
		/export/servers/zookeeper-3.4.9/bin/zServer.sh start 
zookeeper查看启动状态
		/export/servers/zookeeper-3.4.9/bin/zServer.sh status
```

#### hadoop命令

```shell
启动hadoop集群
		# 会登录进所有的worker启动相关进行, 也可以手动进行, 但是没必要
		/export/servers/hadoop-3.1.1/sbin/start-dfs.sh
		/export/servers/hadoop-3.1.1/sbin/start-yarn.sh
		mapred --daemon start historyserver
成功检查
		此时便可以通过如下三个URL访问Hadoop了
		- HDFS: `http://192.168.174.100:50070/dfshealth.html#tab-overview`
		- Yarn: `http://192.168.174.100:8088/cluster`
```

#### hdfs命令

```shell

HDFS
	--shell的操作命令
	hdfs dfs -ls / 查看根路径下面的文件或者文件夹
	hdfs dfs -mkdir -p /xx/xxx	在hdfs上面递归的创建文件夹
	hdfs dfs -moveFromLocal sourceDir(本地磁盘的文件或者文件夹的路径) destDir（hdfs的路径）
	hdfs dfs -mv hdfsourceDir hdfsDestDir 移动hdfs上的文件位置
	hdfs dfs -put localDir hdfsDir	将本地文件系统的文件或者文件夹放到hdfs上面去
	hdfs dfs -appendToFile	a.txt b.txt /hello.txt 将多个小文件合并为一个大文件
	hdfs dfs -cat hdfsFile	查看hdfs的文件内容
	hdfs dfs -cp hdfsSourceDir hdfsDestDir	拷贝文件或者文件夹
	hdfs dfs -rm [-r] File[Dir]	（递归）删除文件或者文件夹
	
	hdfs的权限管理两个命令
	hdfs dfs -chmod -R 777 /xxx 更改文件（目录）访问权限
	hdfs dfs -chown	-R hadoop:hadoop /xxx 更改文件（目录）组访问权限 
	
	--修改权限
	关闭集群
	cd /export/servers/hadoop-3.1.1/etc/hadoop
	vim hdfs-site.xml
		三台机器同步
	<property>
		<name>dfs.permissions.enabled</name>
		<value>true</value>
	</property>
	--文件合并
	可以通过命令行将很多的 hdfs 文件合并成一个大文件下载到本地
	hdfs dfs -getmerge /config/*.xml ./hello.xml
```

#### 集群拷贝命令

```shell
	scp hdfs-site.xml node02:$PWD
	scp hdfs-site.xml node03:$PWD
```

#### hive启动命令	

```shell
Hive数据仓库
	--启动HIve
	bin/hive
```

#### hbase启动命令

```shell
hbase启动
	--启动hbase集群
	hbase-all.sh
```

#### flink集群启动命令及任务启动命令

```shell
flink
	--启动flink
	start-cluster.sh
	./flink run -c com.guigu.wc.StreamWordCount /runjar/FlinkTourtourial-1.0-SNAPSHOT.jar /applog/flink/input.txt /applog/flink/output.csv
	./flink run -m yarn-cluster -c com.guigu.wc.StreamWordCount /runjar/FlinkTourtourial-1.0-SNAPSHOT.jar /applog/flink/input.txt /applog/flink/output.csv
	
	./flink run -c com.guigu.wc.StreamWordCount_socket /runjar/FlinkTourtourial-1.0-SNAPSHOT.jar 192.168.75.100 7777 /applog/flink/output.csv
	./flink run -m yarn-cluster -c com.guigu.wc.StreamWordCount_socket /runjar/FlinkTourtourial-1.0-SNAPSHOT.jar 192.168.75.100 7777 /applog/flink/output.csv
	bin/flink run /export/servers/flink-1.6.0/examples/batch/WordCount.jar --input hdfs://node-1:9000/test/input/wordcount.txt --output hdfs://node-1:9000/test/output/result.txt
```

#### kafka后台启动命令

```shell

kafka启动
	--启动
	bin/kafka-server-start.sh -daemon config/server.properties
```

#### mapreduce

```shell
--任务启动hive
yarn jar hadoop_hdfs_operate‐1.0‐SNAPSHOT.jar
cn.itcast.hdfs.demo1.JobMain
--应用提交
hadoop jar jar_name MainClass
```



#### spark命令

##### Local模式

```shell
--启动 Local 环境 
bin/spark-shell
--验证Local环境是否成功
http://192.168.75.100:4040/jobs/
--程序验证
scala> sc.textFile("data/word.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
--结果
res1: Array[(String, Int)] = Array((Hello,2), (Scala,1), (Spark,1))
--应用提交
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master local[2] \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10
```

##### standalone模式

```shell
--启动历史服务
sbin/start-history-server.sh
--启动集群
sbin/start-all.sh
--启动节点--高可用主备模式
sbin/start-master.sh
--停止集群
sbin/stop-all.sh
--应用提交
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://linux1:7077 \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10
--提交应用到高可用集群
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://linux1:7077,linux2:7077 \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10

```

##### Yarn模式

```shell
--启动历史服务
sbin/start-history-server.sh
--启动集群
sbin/start-all.sh
--应用提交
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode client \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10
```

##### window模式

```shell
--启动
bin 目录中的 spark-shell.cmd
--应用提交
spark-submit --class org.apache.spark.examples.SparkPi --master
local[2] ../examples/jars/spark-examples_2.12-3.0.0.jar 10
```

## 问题

### kibana关闭

```shell
ps -ef | grep '.*node/bin/node.*src/cli'
username   5989  2989  1 11:33 pts/1    00:00:05 ./../node/bin/node ./../src/cli
kill -9 5989
```

## Other

### flume与storm了解

- Apache Flume 是一种用于收集大量流数据，尤其是日志的服务。Flume 使用称为数据接收器的机制将数据推送给消费者。Flume 可以开箱即用地将数据推送到许多流行的接收器，包括 HDFS、HBase、Cassandra 和一些关系数据库。

- Apache Storm 涉及流数据。它是批处理和流处理之间的桥梁，Hadoop 本身并不是设计用来处理的。Storm 持续运行，处理传入的数据流并将其分成批次，因此 Hadoop 可以更轻松地摄取它。数据源称为 spout，每个处理节点都是一个 bolt。Bolt 对数据执行计算和处理，包括将输出推送到数据存储和其他服务。

- 如果您需要开箱即用的东西，请选择 Flume，一旦您决定是推还是拉更有意义。如果流数据现在只是您已经开发的 Hadoop 环境的一个小附加组件，Storm 是一个不错的选择。

- 可以使用风暴将日志数据摄取到 Hadoop 集群中

- 我们可以用风暴代替水槽


​		
​		
​		
​		
​		
​		
​		
​		
​		
​	
