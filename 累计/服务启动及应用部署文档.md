---
typora-root-url: img_install
---

# 启动命令

## zookeeper命令

```powershell
zookeeper启动命令         
        /export/servers/zookeeper-3.4.9/bin/zServer.sh start 
zookeeper查看启动状态
        /export/servers/zookeeper-3.4.9/bin/zServer.sh status
```

## hadoop命令

```powershell
启动hadoop集群
        # 会登录进所有的worker启动相关进行, 也可以手动进行, 但是没必要
        /export/servers/hadoop-3.1.1/sbin/start-dfs.sh
        /export/servers/hadoop-3.1.1/sbin/start-yarn.sh
        mapred --daemon start historyserver
成功检查
        此时便可以通过如下三个URL访问Hadoop了
        - HDFS: `http://192.168.174.100:50070/dfshealth.html#tab-overview`
        - Yarn: `http://192.168.174.100:8088/cluster`
```

## hdfs命令

```powershell

HDFS
    --启动命令
      sbin/start-dfs.sh
      sbin/start-yarn.sh
    --shell的操作命令
    hdfs dfs -ls / 查看根路径下面的文件或者文件夹
    hdfs dfs -mkdir -p /xx/xxx    在hdfs上面递归的创建文件夹
    hdfs dfs -moveFromLocal sourceDir(本地磁盘的文件或者文件夹的路径) destDir（hdfs的路径）
    hdfs dfs -mv hdfsourceDir hdfsDestDir 移动hdfs上的文件位置
    hdfs dfs -put localDir hdfsDir    将本地文件系统的文件或者文件夹放到hdfs上面去
    hdfs dfs -appendToFile    a.txt b.txt /hello.txt 将多个小文件合并为一个大文件
    hdfs dfs -cat hdfsFile    查看hdfs的文件内容
    hdfs dfs -cp hdfsSourceDir hdfsDestDir    拷贝文件或者文件夹
    hdfs dfs -rm [-r] File[Dir]    （递归）删除文件或者文件夹
    hadoop fs -rm -r -skipTrash /folder_name   删除目录folder_name
    --从hdfs拷贝到本地
    hadoop fs -du -s -h /jinguo
    hadoop fs -du -h /jinguo
    ---setrep：设置 HDFS 中文件的副本数量
    hadoop fs -setrep 10 /jinguo/shuguo.txt
    
    hadoop fs -copyToLocal /sanguo/shuguo.txt ./  
    hadoop fs -get/sanguo/shuguo.txt ./shuguo2.txt
    --du 统计文件夹的大小信息
    
    hdfs的权限管理两个命令
    hdfs dfs -chmod -R 777 /xxx 更改文件（目录）访问权限
    hdfs dfs -chown    -R hadoop:hadoop /xxx 更改文件（目录）组访问权限 
    
    --修改权限
    关闭集群
    cd /export/servers/hadoop-3.1.1/etc/hadoop
    vim hdfs-site.xml
        三台机器同步
    <property>
        <name>dfs.permissions.enabled</name>
        <value>true</value>
    </property>
    --文件合并
    可以通过命令行将很多的 hdfs 文件合并成一个大文件下载到本地
    hdfs dfs -getmerge /config/*.xml ./hello.xml
```

## 集群拷贝命令

```powershell
    scp hdfs-site.xml node02:$PWD
    scp hdfs-site.xml node03:$PWD
```

## hive启动命令    

```powershell
Hive数据仓库
    --启动HIve
    bin/hive
```

## hbase启动命令

```powershell
hbase启动
    --启动hbase集群
    hbase-all.sh
```

## flink集群启动命令及任务启动命令

```powershell
flink
    --启动flink
    start-cluster.sh
    ./flink run -c com.guigu.wc.StreamWordCount /runjar/FlinkTourtourial-1.0-SNAPSHOT.jar /applog/flink/input.txt /applog/flink/output.csv
    ./flink run -m yarn-cluster -c com.guigu.wc.StreamWordCount /runjar/FlinkTourtourial-1.0-SNAPSHOT.jar /applog/flink/input.txt /applog/flink/output.csv
    
    ./flink run -c com.guigu.wc.StreamWordCount_socket /runjar/FlinkTourtourial-1.0-SNAPSHOT.jar 192.168.75.100 7777 /applog/flink/output.csv
    ./flink run -m yarn-cluster -c com.guigu.wc.StreamWordCount_socket /runjar/FlinkTourtourial-1.0-SNAPSHOT.jar 192.168.75.100 7777 /applog/flink/output.csv
    bin/flink run /export/servers/flink-1.6.0/examples/batch/WordCount.jar --input hdfs://node-1:9000/test/input/wordcount.txt --output hdfs://node-1:9000/test/output/result.txt
```

## kafka后台启动命令

```powershell

kafka启动
    --启动
    bin/kafka-server-start.sh -daemon config/server.properties
```

## mapreduce

```powershell
--任务启动hive
yarn jar hadoop_hdfs_operate‐1.0‐SNAPSHOT.jar
cn.itcast.hdfs.demo1.JobMain
--应用提交
hadoop jar jar_name MainClass
```

## spark命令

### Local模式

```powershell
--启动 Local 环境 
bin/spark-shell
--验证Local环境是否成功
http://192.168.75.100:4040/jobs/
--程序验证
scala> sc.textFile("data/word.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
--结果
res1: Array[(String, Int)] = Array((Hello,2), (Scala,1), (Spark,1))
--应用提交
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master local[2] \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10
```

### standalone模式

```powershell
--启动历史服务
sbin/start-history-server.sh
--启动集群
sbin/start-all.sh
--启动节点--高可用主备模式
sbin/start-master.sh
--停止集群
sbin/stop-all.sh
--应用提交
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://linux1:7077 \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10
--提交应用到高可用集群
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://linux1:7077,linux2:7077 \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10

```

## Yarn模式

```powershell
--启动历史服务
sbin/start-history-server.sh
--启动集群
sbin/start-all.sh
--应用提交
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode client \
./examples/jars/spark-examples_2.12-3.0.0.jar \
10
```

## window模式

```powershell
--启动
bin 目录中的 spark-shell.cmd
--应用提交
spark-submit --class org.apache.spark.examples.SparkPi --master
local[2] ../examples/jars/spark-examples_2.12-3.0.0.jar 10
```

## flume启动

### 网络采集

```shell
bin/flume-ng agent -c conf -f conf/netcat-logger.conf -n a1 -Dflume.root.logger=INFO,console
```

- -c conf 指定flume自身的配置文件所在目录
- -f conf/netcat-logger.conf 指定我们所描述的采集方案
- -n al 指定我们agent的名字

netcat-logger.conf

```shell
# 定义这个agent中各组件的名字
a1.sources = r1
a1.sinks = k1
a1.channels = c1
# 描述和配置source组件：r1
a1.sources.r1.type = netcat
a1.sources.r1.bind = 192.168.174.
a1.sources.r1.port = 44444
# 描述和配置sink组件：k1
a1.sinks.k1.type = logger
# 描述和配置channel组件，此处使用是内存缓存的方式
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100
# 描述和配置source channel sink之间的连接关系
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

```



### 目录采集

spooldir.cof

```shell
# 配置三个组件的名称
a1.sources = r1
a1.sinks = k1
a1.channels = c1
# 注意不能网监控目录中丢同名文件
# 配置source
a1.sources.r1.type = spooldir
al.sources.r1.spoolDir = /export/servers/dirfile
a1.sources.r1.fileHeader = true
# sink
a1.sources.r1.type = spooldir
a1.sources.r1.spoolDir = /export/servers/dirfile
a1.sources.r1.fileHeader = true
# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = c1
a1.sinks.k1.hdfs.path = hdfs://node01:8020/spooldir/files/%y-%m-%d/%H%M/
a1.sinks.k1.hdfs.filePrefix = events-
# 使用文件滚动
a1.sinks.k1.hdfs.round = true
# hdfs文件夹的滚动周期
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = minute
# 每个3秒钟产生一个新文件
a1.sinks.k1.hdfs.rollInterval = 3
# 文件（hdfs的临时文件）大小达到20字节产生新文件
a1.sinks.k1.hdfs.rollSize = 20
# hdfs临时文件大小达到5个event时产生新文件
a1.sinks.k1.hdfs.rollCount = 5
# 放入队列的event的个数，batchSize<transactionCapacity<capacity
a1.sinks.k1.hdfs.batchSize = 1
# 是否使用本地时间戳
a1.sinks.k1.hdfs.useLocalTimeStamp = true
#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本
a1.sinks.k1.hdfs.fileType = DataStream
# 配置channel
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100
# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

启动实例

```shell
bin/flume-ng agent -c ./conf -f ./conf/sqooldir.conf -n a1 -Dflume.root.logger=INFO,console
```



### 启动报错由于hbase

![](/flume.png)
